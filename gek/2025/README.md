 
# Задания к ГЭК по бизнес-информатике 

## Задание 1. Анализ больших объемов экономических данных с использованием Hadoop и Spark

### Описание
Проанализировать большой объем экономических данных, представленных во внешнем файле или CSV-документе, с использованием распределенной файловой системы Hadoop HDFS и инструментов обработки данных Spark, PySpark или PySpark SQL. Необходимо извлечь, обработать и проанализировать данные для выявления закономерностей и тенденций, а также представить результаты в виде визуализаций и статистических отчетов.

### Исходные данные
- **Формат:** CSV-файлы или JSON
- **Особенность:** Объем данных превышает возможности локальной обработки, что требует распределенной обработки в Hadoop

### Задачи
- [x] Подключиться к Hadoop и загрузить данные в HDFS
- [x] Обработать данные с использованием Spark/PySpark
- [x] Проанализировать данные и выявить тенденций
- [x] Визуализировать данные
- [x] Сохранить результаты
- [x] Автоматизировать процесс

### Ресурсы для выполнения
- **Пример выполнения:** [Moodle задание](http://95.131.149.21/moodle/mod/assign/view.php?id=919)

### Виртуальная гостевая ОС
- **Система:** Ubuntu 24.04 ds_mgpu_Hadoop3+spark_3_4
- **Ссылка для скачивания:** [Яндекс.Диск](https://disk.yandex.ru/d/zEKP6GY6Nosaxg)

### Матери [Пошаговая инструкция (steps_z1.pdf)](https://github.com/BosenkoTM/BigDataWork/blob/main/gek/2025/data/z1/steps_z1.pdf [Данные GDP.csv](https://github.com/BosenkoTM/BigDataWork/blob/main/gek/2025/data/z1/GDP.csv)
- Notebook с примером работы](https://github.com/BosenkoTM/BigDataWork/blob/main/gek/2025/data/z1/work_with_data_2024.ipynb)

##  Задание 2. Анализ финансовых данных с использованием Hadoop и Hive

### Описание
Разработать и выполнить анализ большого объема финансовых данных с использованием распределенной файловой системы Hadoop и инструмента Hive. Для обработки данных применяется язык запросов HiveQL. Требуется извлечь, обработать и выполнить аналитические запросы с целью выявления закономерностей, тенденций, и создания визуализаций на основе предоставленных данных.

### Исходные данные
- **Формат:** CSV или JSON-документы
- **Особенность:** Объем данных превышает возможности локальной обработки, что требует использования распределенной файловой системы HDFS

### Задачи
- [x] Создать структуру данных в Hive
- [x] Загрузить данные в Hive
- [x] Обработать данные с использованием HiveQL
- [x] Провести анализ финансовых данных
- [x] Оптимизировать запросы

### Ресурсы для выполнения
- **Основной репозиторий:** [Cloudera Quickstart](https://github.com/BosenkoTM/cloudera-quickstart)
- **Альтернативный ресурс:** [Практические упражнения по Hive](https://github.com/BosenkoTM/BigDataAnalitic_Practice/tree/main/exercises/winter_semester_2021-2022/02_hive)
- **Алгоритм решения:** [Exercise_2.pdf](https://github.com/BosenkoTM/BigDataAnalitic_Practice/blob/main/solutions/winter_semester_2021-2022/02_hive/Exercise_2.pdf)

##  Задание 3. Архитектура хранилища больших данных для МСБ

### Описание
Разработать архитектуру хранилища больших данных для заданного сценария использования в развитии малого и среднего бизнеса на основе использования технологии Big Data.

### Задачи

#### 1. Анализ требований
- Определить источники данных
- Выявить типы данных (структурированные, полуструктурированные, неструктурированные)
- Оценить объемы данных и скорость их поступления
- Определить требования к аналитике и отчетности

#### 2. Выбор компонентов архитектуры
- Выбрать систему распределенного хранения (например, Hadoop HDFS, Amazon S3)
- Определить систему обработки данных (например, Apache Spark, Flink)
- Выбрать систему управления метаданными (например, Apache Atlas)
- Определить инструменты для ETL процессов (например, Apache NiFi, Talend)
- Выбрать инструменты для визуализации и аналитики (например, Tableau, Power BI)

#### 3. Проектирование архитектуры
- Разработать схему потоков данных
- Определить компоненты для обеспечения безопасности и управления доступом
- Спроектировать систему мониторинга и логирования
- Разработать стратегию масштабирования

#### 4. Создание диаграмм архитектуры
- Использовать draw.io для создания визуального представления архитектуры
- Включить все основные компоненты и связи между ними

#### 5. Описание компонентов
- Для каждого компонента архитектуры предоставить краткое описание его роли и функций

#### 6. Обоснование выбора
- Объяснить причины выбора конкретных технологий и компонентов

#### 7. Производительность и масштабируемость
- Описать, как архитектура обеспечивает высокую производительность и масштабируемость

### Материалы
-Пошаговая инструкция (steps_z3.pdf)](https://github.com/BosenkoTM/BigDataWork/blob/main/gek/2025/data/z3/steps_z3.pdf)

---

## Рекомендации по выполнению

### Технические требования
- Знание основ работы с Hadoop HDFS
- Понимание принципов работы Apache Spark и PySpark
- Навыки работы с HiveQL
- Умение создавать архитектурные диаграммы
- Знание принципов проектирования систем больших данных

### Критерии оценки
1. **Техническая реализация** - корректность использования инструментов Big Data
2. **Качество анализа** - глубина выявленных закономерностей и тенденций
3. **Визуализация** - качество представления результатов
4. **Архитектурное решение** - обоснованность выбора компонентов и их интеграции
5. **Документация** - полнота описания процесса и результатов

### Полезные ссылки
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Apache Hive Documentation](https://hive.apache.org/)
- [Hadoop Documentation](https://hadoop.apache.org/docs/stable/)
- [Draw.io](https://app.diagrams.net/)

---

*Удачи в выполнении заданий!*
