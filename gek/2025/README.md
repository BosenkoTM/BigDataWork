 
# Задания к ГЭК по бизнес-информатике 

## Задание 1. Анализ больших объемов экономических данных с использованием Hadoop и Spark

### Описание
Проанализировать большой объем экономических данных, представленных во внешнем файле или CSV-документе, с использованием распределенной файловой системы Hadoop HDFS и инструментов обработки данных Spark, PySpark или PySpark SQL. Необходимо извлечь, обработать и проанализировать данные для выявления закономерностей и тенденций, а также представить результаты в виде визуализаций и статистических отчетов.

## Технические требования

**Программная среда:**
- Виртуальная ОС: Ubuntu 24.04 ds_mgpu_Hadoop3+spark_3_4.
- Доступ к образу: https://disk.yandex.ru/d/zEKP6GY6Nosaxg.
- Hadoop HDFS для распределенного хранения данных.
- Apache Spark 3.4 с поддержкой PySpark и PySpark SQL.
- Python библиотеки: pandas, matplotlib, seaborn для визуализации.

### Исходные данные
- **Формат:** CSV-файлы или JSON.
- **Особенность:** Объем данных превышает возможности локальной обработки, что требует распределенной обработки в Hadoop.

### Задачи
- [x] Подключиться к Hadoop и загрузить данные в HDFS.
- [x] Обработать данные с использованием Spark/PySpark.
- [x] Проанализировать данные и выявить тенденций.
- [x] Визуализировать данные.
- [x] Сохранить результаты.
- [x] Автоматизировать процесс.

## Варианты заданий для выполнения

| № | Тип трансформации | Источник данных | Аналитические задачи |
|---|---|---|---|
| 1 | Очистка, агрегация, объединение | [Countries of the World 2023](https://www.kaggle.com/datasets/nelgiriyewithana/countries-of-the-world-2023) | Сравнительный анализ социально-экономических показателей стран, выявление корреляций между ВВП, населением, уровнем образования |
| 2 | Очистка, нормализация, временной срез | [Lending Club Loan Data](https://www.kaggle.com/datasets/wordsforthewise/lending-club) | Анализ кредитного риска, построение модели предсказания дефолтов, сегментация заемщиков |
| 3 | Обработка временных рядов, фильтрация | [S&P 500 Stock Prices](https://www.kaggle.com/datasets/camnugent/sandp500) | Анализ динамики фондового рынка, выявление трендов, расчет технических индикаторов |
| 4 | Объединение, категоризация, извлечение признаков | [US Macro-Economic Factors 2002-2022](https://www.kaggle.com/datasets/sagarvarandekar/macroeconomic-factors-affecting-us-housing-prices) | Анализ макроэкономических факторов, влияющих на рынок недвижимости США |
| 5 | Текстовая обработка, тональный анализ | [Financial News Sentiment](https://www.kaggle.com/datasets/aaron7sun/stocknews) | Анализ влияния новостного сентимента на колебания финансовых рынков |

## Этапы выполнения задания

### 1. Подготовка инфраструктуры
- Запуск виртуальной машины с предустановленной средой Hadoop+Spark.
- Проверка работоспособности HDFS и Spark-кластера.
- Загрузка выбранного датасета из Kaggle.

### 2. Загрузка данных в HDFS
```bash
# Создание директории в HDFS
hdfs dfs -mkdir /user/data/economic_analysis

# Загрузка данных в HDFS
hdfs dfs -put dataset.csv /user/data/economic_analysis/
```

### 3. Обработка данных с использованием PySpark
- Подключение к Spark-кластеру.
- Чтение данных из HDFS в Spark DataFrame.
- Выполнение операций очистки и трансформации данных.
- Применение специфических операций согласно варианту задания.

### 4. Аналитическая обработка
- Выполнение статистического анализа данных.
- Выявление закономерностей и тенденций.
- Применение методов машинного обучения (при необходимости).
- Расчет ключевых экономических показателей.

### 5. Визуализация результатов
- Создание информативных графиков и диаграмм.
- Построение дашбордов для представления результатов.
- Интерпретация полученных визуализаций.

### 6. Сохранение результатов
- Сохранение обработанных данных обратно в HDFS.
- Экспорт результатов анализа в различных форматах.
- Создание итогового отчета.

### 7. Автоматизация процесса
- Создание скриптов для воспроизводимости анализа.
- Документирование процедур обработки данных.
- Оптимизация производительности Spark-приложений.

## Методические материалы

**Примеры выполнения:**
- [Пример решения 1 (PDF)](https://github.com/BosenkoTM/BigDataWork/blob/main/gek/2025/data/z1/steps_z1_01_2024.pdf)
- [Jupyter Notebook 1](https://github.com/BosenkoTM/BigDataWork/blob/main/gek/2025/data/z1/work_with_data_01_2024.ipynb)
- [Пример решения 2 (PDF)](https://github.com/BosenkoTM/BigDataWork/blob/main/gek/2025/data/z1/steps_z1_02_2025.pdf)
- [Jupyter Notebook 2](https://github.com/BosenkoTM/BigDataWork/blob/main/gek/2025/data/z1/work_with_data_02_2025.ipynb)

## Критерии оценки

1. **Техническая реализация (40%)**
   - Корректность настройки Hadoop/Spark окружения.
   - Эффективность операций с большими данными.
   - Качество кода PySpark.

2. **Аналитическая глубина (35%)**
   - Обоснованность выбора методов анализа.
   - Качество интерпретации результатов.
   - Выявление значимых закономерностей.

3. **Визуализация и презентация (25%)**
   - Информативность графиков и диаграмм.
   - Качество итогового отчета.
   - Воспроизводимость результатов.

## Ожидаемые результаты

По завершении задания студент должен предоставить:
- Jupyter Notebook с полным циклом анализа данных.
- Аналитический отчет с выводами и рекомендациями.
- Набор визуализаций, демонстрирующих ключевые находки.
- Документацию по воспроизведению анализа.

##  Задание 2. Анализ финансовых данных с использованием Hadoop и Hive

### Описание
Разработать и выполнить анализ большого объема финансовых данных с использованием распределенной файловой системы Hadoop и инструмента Hive. Для обработки данных применяется язык запросов HiveQL. Требуется извлечь, обработать и выполнить аналитические запросы с целью выявления закономерностей, тенденций, и создания визуализаций на основе предоставленных данных.

### Исходные данные
- **Формат:** CSV или JSON-документы
- **Особенность:** Объем данных превышает возможности локальной обработки, что требует использования распределенной файловой системы HDFS

### Задачи
- [x] Создать структуру данных в Hive.
- [x] Загрузить данные в Hive.
- [x] Обработать данные с использованием HiveQL.
- [x] Провести анализ финансовых данных.
- [x] Оптимизировать запросы.

### Ресурсы для выполнения
- **Основной репозиторий:** [Cloudera Quickstart](https://github.com/BosenkoTM/cloudera-quickstart)
- **Альтернативный ресурс:** [Практические упражнения по Hive](https://github.com/BosenkoTM/BigDataAnalitic_Practice/tree/main/exercises/winter_semester_2021-2022/02_hive)
- **Алгоритм решения:** [Exercise_2.pdf](https://github.com/BosenkoTM/BigDataAnalitic_Practice/blob/main/solutions/winter_semester_2021-2022/02_hive/Exercise_2.pdf)

### Технические требования
- Знание основ работы с Hadoop HDFS.
- Понимание принципов работы Apache Spark и PySpark.
- Навыки работы с HiveQL.
- Умение создавать архитектурные диаграммы.
- Знание принципов проектирования систем больших данных.

---

##  Задание 3. Архитектура хранилища больших данных

### Описание
Разработать архитектуру хранилища больших данных для заданного сценария использования в развитии малого и среднего бизнеса на основе использования технологии Big Data.

### Задачи

#### 1. Анализ требований
- Определить источники данных.
- Выявить типы данных (структурированные, полуструктурированные, неструктурированные).
- Оценить объемы данных и скорость их поступления.
- Определить требования к аналитике и отчетности.

#### 2. Выбор компонентов архитектуры
- Выбрать систему распределенного хранения (например, Hadoop HDFS, Amazon S3).
- Определить систему обработки данных (например, Apache Spark, Flink).
- Выбрать систему управления метаданными (например, Apache Atlas).
- Определить инструменты для ETL процессов (например, Apache NiFi, Talend).
- Выбрать инструменты для визуализации и аналитики (например, Tableau, Power BI).

#### 3. Проектирование архитектуры
- Разработать схему потоков данных.
- Определить компоненты для обеспечения безопасности и управления доступом.
- Спроектировать систему мониторинга и логирования.
- Разработать стратегию масштабирования.

#### 4. Создание диаграмм архитектуры
- Использовать draw.io для создания визуального представления архитектуры.
- Включить все основные компоненты и связи между ними.

#### 5. Описание компонентов
- Для каждого компонента архитектуры предоставить краткое описание его роли и функций.

#### 6. Обоснование выбора
- Объяснить причины выбора конкретных технологий и компонентов.

#### 7. Производительность и масштабируемость
- Описать, как архитектура обеспечивает высокую производительность и масштабируемость.

### Материалы
-Пошаговая инструкция [steps_z3.pdf](https://github.com/BosenkoTM/BigDataWork/blob/main/gek/2025/data/z3/steps_z3.pdf)

### Варианты задания 

#### Вариант 3.1. Крупный онлайн-ритейлер

- **Объем данных:** 500 ТБ в год, рост 50% ежегодно.
- **Скорость получения данных:** до 5000 транзакций в секунду.  
- **Типы данных:**
  - 60% структурированные.
  - 30% полуструктурированные.
  - 10% неструктурированные.
- **Требования к обработке:**
  - Анализ поведения пользователей в реальном времени.
  - Прогнозирование спроса.
- **Доступность:** 99.99%, время отклика <5 секунд . 
- **Безопасность:** Шифрование, соответствие [152-ФЗ](https://www.consultant.ru/document/cons_doc_LAW_61801/) и [PCI DSS](https://www.pcisecuritystandards.org/ )

---

#### Вариант 3.2. Финансовая компания

- **Объем данных:** 100 ТБ в год, рост 30% ежегодно.
- **Скорость получения данных:** до 1000 транзакций в секунду.  
- **Типы данных:**
  - 80% структурированные.
  - 15% полуструктурированные.
  - 5% неструктурированные.
- **Требования к обработке:**
  - Выявление мошенничества в реальном времени.
  - Оценка кредитных рисков.
- **Доступность:** 99.999%, время отклика <1 секунда.  
- **Безопасность:** Сквозное шифрование, строгое соответствие [152-ФЗ](https://www.consultant.ru/document/cons_doc_LAW_61801/) и требованиям [ЦБ РФ](https://www.cbr.ru/ )

---

#### Вариант 3.3. Телекоммуникационная компания

- **Объем данных:** 1 ПБ в год, рост 40% ежегодно.  
- **Скорость получения данных:** до 10000 событий в секунду.  
- **Типы данных:**
  - 40% структурированные.
  - 50% полуструктурированные.
  - 10% неструктурированные.
- **Требования к обработке:**
  - Анализ качества связи.
  - Прогнозирование нагрузки на сеть.
- **Доступность:** 99.99%, время отклика <10 секунд.  
- **Безопасность:** Шифрование, соответствие [152-ФЗ](https://www.consultant.ru/document/cons_doc_LAW_61801/) и отраслевым стандартам

---

#### Вариант 3.4. Логистическая компания

- **Объем данных:** 50 ТБ в год, рост 25% ежегодно.  
- **Скорость получения:** до 500 событий в секунду.  
- **Типы данных:**
  - 70% структурированные.
  - 20% полуструктурированные.
  - 10% неструктурированные.
- **Требования к обработке:**
  - Оптимизация маршрутов в реальном времени.
  - Прогнозирование спроса на услуги.
- **Доступность:** 99.9%, время отклика <30 секунд.  
- **Безопасность:** Базовое шифрование, соответствие [152-ФЗ](https://www.consultant.ru/document/cons_doc_LAW_61801/)

---

## Рекомендации по выполнению

### Критерии оценки
1. **Техническая реализация** - корректность использования инструментов Big Data.
2. **Качество анализа** - глубина выявленных закономерностей и тенденций.
3. **Визуализация** - качество представления результатов.
4. **Архитектурное решение** - обоснованность выбора компонентов и их интеграции.
5. **Документация** - полнота описания процесса и результатов.

### Полезные ссылки
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Apache Hive Documentation](https://hive.apache.org/)
- [Hadoop Documentation](https://hadoop.apache.org/docs/stable/)
- [Draw.io](https://app.diagrams.net/)

---

## Задание 4

### Задание 4.1 Используя учебную базу данных и кроссплатформенный менеджер **DBeaver**, выполните SQL-запросы, которые позволят определить цену заказа на каждую дату.  

**Требования:**
- Отсортируйте результаты по `id` заказа.
- Выведите:  
  - `id` заказа.
  - дату заказа.
  - цену единицы товара.  

Выполните **два варианта запроса**:
1. **Без использования** оператора `INNER JOIN`
2. **С использованием** оператора `INNER JOIN`

Сравните результаты двух запросов и **убедитесь в их идентичности**.

### Задание 4.2 Используя учебную базу данных и кроссплатформенный менеджер DBeaver, выполните запросы на языке SQL, которые позволят определить, какова цена заказа на каждую дату. Отсортируйте по id заказа. Выведите id заказа, дату заказа и цену единицы товара.
Выполните два варианта запроса: без использования и с использованием оператора `inner join`. Убедитесь, что предложенные вами два запроса дают идентичные результаты.

---

### Прогнозный анализ

Проведите **прогнозный анализ** на данных с **выгрузкой агрегатных значений** и **значимых экономических метрик**. Подготовьте результаты для **визуализации в DataLens** (Yandex DataLens, Tableau и т.д.).

---

### Программное обеспечение

- [PostgreSQL **`16.0` или выше** (Windows / macOS / Linux)](https://www.enterprisedb.com/downloads/postgres-postgresql-downloads)

#### Установка:

1. Установите **PostgreSQL 16.0+** с официального сайта:
   [https://www.enterprisedb.com/downloads/postgres-postgresql-downloads](https://www.enterprisedb.com/downloads/postgres-postgresql-downloads)

2. Загрузите и восстановите одну из следующих учебных баз данных в PostgreSQL:
   - [**data.dump**](https://disk.yandex.ru/d/p3ga3WZpmAw8-Q)
   - [**bi_sql_data_student_dump.sql**](https://disk.yandex.ru/d/yJTa_s86xpbpMQ)

---

#### Примечание

При выполнении запроса обратите внимание на:
- корректность связей между таблицами.
- агрегирование данных при необходимости.
- использование `ORDER BY` по `id` заказа.


*Удачи в выполнении заданий!*
