## Практическая работа 1.2. Обработка данных с использованием Apache Spark и Python (PySpark)
Цель: освоение основ работы с Apache Spark и его интеграцией с Python через библиотеку PySpark. Студенты научатся обрабатывать большие объемы данных, используя распределенные вычисления, а также научатся применять базовые операции с RDD (Resilient Distributed Datasets) и DataFrame, работать с SQL-запросами в Spark SQL, а также визуализировать результаты обработки данных.

### Задачи:
1. **Установить Apache Spark и PySpark.**  
   Настроить рабочую среду для использования PySpark в Python, установить необходимые зависимости и настроить Spark на локальном компьютере или через облачную платформу.

2. **Загрузка данных и их предварительная обработка.**  
   Скачать или подготовить исходные данные для анализа (например, текстовые файлы или CSV). Загружать данные в Spark через RDD или DataFrame, выполнить предварительную обработку: очистка данных, фильтрация, преобразования.

3. **Применение операций с RDD и DataFrame.**  
   Научиться работать с RDD и DataFrame, выполнять такие операции как map, filter, reduce, groupBy, join и другие стандартные операции для обработки данных в распределенной среде.

4. **Применение SQL-запросов через Spark SQL.**  
   Использование SQL-запросов в Spark для извлечения и агрегации данных, создание временных таблиц и выполнение сложных запросов для анализа данных.

5. **Визуализация результатов анализа данных.**  
   Визуализировать полученные результаты с помощью библиотеки Python для визуализации данных (например, matplotlib или seaborn). Построить графики для лучшего представления результатов.

6. **Подготовка отчета.**  
   Оформить отчет по выполненной практике, в котором будет описан процесс выполнения работы, анализ полученных результатов и выводы. Включить ссылки на репозиторий и прикрепить сам отчет в формате PDF или Markdown.


## Индивидуальное задание

| Задание № | Описание задания | Технология/Режим | Детали |
|-----------|------------------|------------------|--------|
| 1 | **WordCount с использованием Hadoop и Spark** | Spark + Hadoop | Подсчет частоты слов в текстовом файле, размещенном на HDFS. |
| 2 | **WordCount с использованием Spark Local** | Spark Local | Подсчет частоты слов в текстовом файле с использованием локального режима работы Spark. |
| 3 | **Визуализация данных WordCount** | Python (matplotlib, seaborn) | Построение гистограммы для отображения 10 наиболее частых слов из анализа. |
| 4 | **SQL-запросы с использованием Spark и Hadoop** | Spark + Hadoop | Выполнение SQL-запросов для извлечения и агрегации данных из CSV-файлов, размещенных в HDFS. |
| 5 | **SQL-запросы с использованием Spark Local** | Spark Local | Выполнение SQL-запросов для извлечения и агрегации данных из локальных CSV-файлов. |
| 6 | **Визуализация распределения цен по странам** | Python (matplotlib, seaborn) | Визуализация распределения цен по странам с использованием boxplot. |
| 7 | **Анализ дохода по странам и штатам с использованием Spark и Hadoop** | Spark + Hadoop | Использование SQL-запроса для агрегации данных по странам и штатам с расчетом общего дохода. |
| 8 | **Анализ дохода по странам и штатам с использованием Spark Local** | Spark Local | Анализ дохода по странам и штатам с использованием SQL-запросов в локальном режиме. |
| 9 | **Реализация MapReduce для подсчета частоты слов** | Spark + Hadoop | Использование MapReduce для подсчета слов в большом тексте, сохраненном в HDFS. |
| 10 | **Использование Spark для работы с большими данными** | Spark + Hadoop | Загрузка и обработка данных из HDFS, применение различных операций Spark для анализа. |
| 11 | **Моделирование с использованием Spark MLlib** | Spark MLlib | Построение и обучение моделей машинного обучения, используя алгоритмы классификации. |
| 12 | **Обработка и агрегация данных с использованием Spark SQL** | Spark SQL | Применение SQL-запросов для обработки больших данных, включая агрегирование и фильтрацию. |
| 13 | **Применение PySpark для обработки больших текстовых файлов** | PySpark | Обработка текстовых данных с использованием PySpark для подсчета частоты слов. |
| 14 | **Оптимизация работы с большими данными в Spark** | Spark + Hadoop | Применение оптимизаций для эффективной работы с большими данными в Spark. |
| 15 | **Использование функций окна в Spark SQL** | Spark SQL | Применение оконных функций для анализа и агрегации данных с использованием SQL-запросов. |
| 16 | **Использование PySpark для анализа временных рядов** | PySpark | Работа с временными рядами и их анализ с использованием PySpark и Spark SQL. |
| 17 | **Применение алгоритмов классификации в Spark MLlib** | Spark MLlib | Реализация моделей классификации с использованием RandomForest и других алгоритмов. |
| 18 | **Работа с DataFrame и RDD в PySpark** | PySpark | Создание и манипуляции с RDD и DataFrame для анализа и обработки данных. |
| 19 | **Работа с большими данными в Spark** | Spark + Hadoop | Обработка и анализ больших объемов данных с использованием возможностей Spark. |
| 20 | **Анализ текстовых данных с использованием RDD** | PySpark | Подсчет слов и анализ текстовых данных с использованием RDD в Spark. |
| 21 | **Визуализация анализа данных с использованием matplotlib** | Python (matplotlib) | Построение визуализаций для анализа данных с использованием matplotlib и seaborn. |
| 22 | **Использование PySpark для обработки CSV-файлов** | PySpark | Загрузка, обработка и агрегация данных из CSV-файлов с использованием PySpark. |
| 23 | **Обработка данных с использованием SQL-запросов в Spark** | Spark SQL | Реализация SQL-запросов для обработки данных и получения аналитических отчетов. |
| 24 | **Применение PySpark для обработки больших текстовых коллекций** | PySpark | Обработка больших коллекций текстовых данных с использованием RDD и DataFrame. |
| 25 | **Моделирование с использованием Spark MLlib (классификация)** | Spark MLlib | Построение и тестирование моделей классификации, таких как Decision Tree и RandomForest. |
| 26 | **Анализ и обработка больших данных с использованием Spark** | Spark + Hadoop | Использование Spark для обработки больших данных, включая работу с большими текстовыми файлами. |
| 27 | **Использование PySpark для анализа и обработки больших данных** | PySpark | Использование функций PySpark для анализа и обработки больших наборов данных. |
| 28 | **Анализ данных с использованием Python и Spark SQL** | PySpark + SQL | Использование SQL-запросов в PySpark для анализа и обработки данных в больших объемах. |
| 29 | **Обработка больших данных с использованием Hadoop и Spark** | Spark + Hadoop | Загрузка и обработка данных с использованием Hadoop для хранения и Spark для обработки. |
| 30 | **Визуализация распределений данных с использованием seaborn** | Python (seaborn) | Создание различных типов графиков для визуализации распределений данных с помощью seaborn. |

### Правила оформления отчета

1. **Название репозитория:**
   Репозиторий должен иметь понятное и лаконичное название, отражающее содержание работы, например: `spark-data-processing-lab1`.

2. **Структура репозитория:**
   В репозитории должны быть следующие файлы:
   - **Код** — основной файл с кодом решения задачи в формате `.py` или `.ipynb`. Все используемые библиотеки должны быть импортированы в коде.
   - **Отчет** — файл с отчетом, оформленный в формате Markdown (например, `report.md`) или PDF, который включает:
     - Введение (цель и задачи работы);
     - Описание методов и шагов, предпринятых для выполнения работы;
     - Пример кода с объяснением ключевых моментов;
     - Анализ полученных результатов;
     - Выводы.
   - **README.md** — файл с общей информацией о репозитории, инструкции по установке и запуску кода, краткое описание работы.

3. **Стиль кода:**
   - Все комментарии в коде должны быть лаконичными и пояснять, что делает каждая часть кода.
   - Код должен быть структурирован, использовать отступы и разделение на логические блоки.
   - Функции и переменные должны иметь осмысленные и описательные имена.

4. **Отчет:**
   - В отчете должно быть описание всех выполненных этапов работы, включая данные, методы обработки и полученные результаты.
   - Для каждой задачи следует предоставить фрагменты кода и краткие пояснения.
   - Включить визуализации результатов (графики, таблицы), если это применимо.
   - В конце отчета должны быть сделаны выводы о выполненной работе.

5. **Ссылка на репозиторий в Moodle:**
   После завершения работы и размещения кода и отчета в репозитории, предоставьте ссылку на репозиторий в Moodle. Убедитесь, что репозиторий является публичным или доступен для преподавателя.