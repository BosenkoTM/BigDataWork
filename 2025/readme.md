## Практическая работа 1.2. Обработка данных с использованием Apache Spark и Python (PySpark)
Цель: освоение основ работы с Apache Spark и его интеграцией с Python через библиотеку PySpark. Студенты научатся обрабатывать большие объемы данных, используя распределенные вычисления, а также научатся применять базовые операции с RDD (Resilient Distributed Datasets) и DataFrame, работать с SQL-запросами в Spark SQL, а также визуализировать результаты обработки данных.

### Задачи:
1. **Установить Apache Spark и PySpark.**  
   Настроить рабочую среду для использования PySpark в Python, установить необходимые зависимости и настроить Spark на локальном компьютере или через облачную платформу.

2. **Загрузка данных и их предварительная обработка.**  
   Скачать или подготовить исходные данные для анализа (например, текстовые файлы или CSV). Загружать данные в Spark через RDD или DataFrame, выполнить предварительную обработку: очистка данных, фильтрация, преобразования.

3. **Применение операций с RDD и DataFrame.**  
   Научиться работать с RDD и DataFrame, выполнять такие операции как map, filter, reduce, groupBy, join и другие стандартные операции для обработки данных в распределенной среде.

4. **Применение SQL-запросов через Spark SQL.**  
   Использование SQL-запросов в Spark для извлечения и агрегации данных, создание временных таблиц и выполнение сложных запросов для анализа данных.

5. **Визуализация результатов анализа данных.**  
   Визуализировать полученные результаты с помощью библиотеки Python для визуализации данных (например, matplotlib или seaborn). Построить графики для лучшего представления результатов.

6. **Подготовка отчета.**  
   Оформить отчет по выполненной практике, в котором будет описан процесс выполнения работы, анализ полученных результатов и выводы. Включить ссылки на репозиторий и прикрепить сам отчет в формате PDF или Markdown.

Алгоритм выполнения задания в Apache Hadoop

Развернуть образ конфигурации  [ds_mgpu_Hadoop3+spark_3_4](http://95.31.0.249/moodle/mod/assign/view.php?id=1357) 

Все дальнейшие действия выполняются пользователем hadoop.
```bash
sudo su - hadoop
```

Шаг 1. Запуск Hadoop.
```bash
start-dfs.sh
```

```bash
start-yarn.sh
```
Шаг 2. Проверка работы Hadoop.
```bash
jps
```

В стандартной конфигурации Hadoop HDFS предоставляет веб-интерфейсы:
-	HDFS NameNode: http://localhost:9870 
-	YARN ResourceManager: http://localhost:8088

установить разрешение на запись для всех пользователей

```bash
hdfs dfs -chmod 775 /user2/hadoop/economic_data
```

разрешить запись только пользователю devops

```bash
hdfs dfs -setfacl -m user:devops:rwx /user3/hadoop/economic_data

Для того чтобы загрузить все файлы из локальной папки `~/Downloads/lab_1_2/data` в HDFS в папку `sparkdir`, выполните следующие шаги:

### Шаг 1. Создание папки в HDFS

Для начала создадим каталог `sparkdir` в HDFS:

```bash
hdfs dfs -mkdir -p /user5/sparkdir
```

Замените `devops` на имя пользователя, если это необходимо.

### Шаг 2. Загрузка файлов в HDFS

Теперь, когда папка `sparkdir` создана, нужно загрузить все файлы из локальной директории в HDFS.

Для этого используйте команду `hdfs dfs -put`:

```bash
hdfs dfs -put /home/hadoop/Downloads/lab_01_2/lab_1_2/data/* /user5/sparkdir/
```

Эта команда загрузит все файлы из папки `/home/hadoop/Downloads/lab_01_2/lab_1_2/data/` в папку `sparkdir` на HDFS.

### Шаг 3. Проверка загрузки

Чтобы убедиться, что файлы были успешно загружены, выполните команду:

```bash
hdfs dfs -ls /user5/sparkdir/
```

Эта команда выведет список всех файлов в каталоге `sparkdir` на HDFS.

Теперь все файлы из `~/Downloads/lab_1_2/data` должны быть успешно загружены в `sparkdir` на HDFS.


Завершение работы с Hadoop

```bash
stop-yarn.sh
```

```bash
stop-dfs.sh
```

Для полной остановки всех Hadoop-демонов:

```bash
stop-all.sh
```

Проверка остановки всех процессов:

```bash
jps
```

## Индивидуальное задание

| Вариант | Задание 1 (Spark + Hadoop) | Задание 2 (Spark Local + SQL) | Задание 3 (Визуализация) |
|---------|---------------------------|------------------------|------------------------|
| 1 | Анализ данных о продажах: загрузить sales.csv в HDFS, подсчитать общую выручку по категориям товаров | SQL-анализ: определить топ-10 товаров по выручке, среднюю стоимость заказа по месяцам | Построить столбчатую диаграмму продаж по месяцам с линией тренда |
| 2 | Обработка транзакций: загрузить transactions.csv в HDFS, найти аномальные транзакции | SQL-анализ: рассчитать средний чек по дням недели и временным интервалам | Визуализировать распределение транзакций по времени суток |
| 3 | Анализ складских остатков: загрузить inventory.csv в HDFS, определить неликвидные товары | SQL-анализ: выявить товары с критически низким запасом, рассчитать оборачиваемость | Создать тепловую карту товарных остатков по категориям |
| 4 | Анализ поставщиков: загрузить suppliers.csv в HDFS, оценить надежность поставщиков | SQL-анализ: рассчитать среднее время поставки и процент задержек по поставщикам | Визуализировать рейтинг поставщиков по ключевым метрикам |
| 5 | Анализ возвратов: загрузить returns.csv в HDFS, определить причины возвратов | SQL-анализ: рассчитать процент возвратов по категориям и производителям | Построить круговую диаграмму причин возвратов |
| 6 | Анализ программы лояльности: загрузить loyalty.csv в HDFS, оценить эффективность программы | SQL-анализ: определить корреляцию между баллами лояльности и частотой покупок | Визуализировать распределение клиентов по уровням лояльности |
| 7 | Анализ онлайн-корзин: загрузить carts.csv в HDFS, исследовать брошенные корзины | SQL-анализ: определить товары, часто покупаемые вместе, и среднее время до покупки | Построить граф связей между товарами |
| 8 | Анализ акций: загрузить promotions.csv в HDFS, оценить влияние на продажи | SQL-анализ: рассчитать ROI акций по разным категориям товаров | Визуализировать эффективность разных типов акций |
| 9 | Анализ сезонности: загрузить seasonal_sales.csv в HDFS, выявить сезонные тренды | SQL-анализ: рассчитать сезонные коэффициенты по категориям товаров | Построить график сезонности продаж |
| 10 | Анализ цен: загрузить pricing.csv в HDFS, исследовать ценовую эластичность | SQL-анализ: определить оптимальные ценовые диапазоны по категориям | Визуализировать зависимость спроса от цены |
| 11 | Анализ доставки: загрузить delivery.csv в HDFS, оценить эффективность логистики | SQL-анализ: рассчитать стоимость доставки по регионам и типам доставки | Построить карту загруженности по регионам |
| 12 | Анализ отзывов: загрузить reviews.csv в HDFS, классифицировать отзывы | SQL-анализ: определить средний рейтинг по категориям и брендам | Визуализировать распределение оценок |
| 13 | Анализ подписок: загрузить subscriptions.csv в HDFS, исследовать отток | SQL-анализ: рассчитать lifetime value подписчиков и причины отказов | Построить воронку конверсии подписчиков |
| 14 | Анализ B2B продаж: загрузить b2b_sales.csv в HDFS, сегментировать корпоративных клиентов | SQL-анализ: определить средний размер оптовой закупки по отраслям | Визуализировать структуру B2B продаж |
| 15 | Анализ маркетплейса: загрузить marketplace.csv в HDFS, оценить эффективность площадок | SQL-анализ: рассчитать комиссии и прибыль по площадкам | Построить сравнительный анализ площадок |
| 16 | Анализ скидок: загрузить discounts.csv в HDFS, оценить эффективность скидок | SQL-анализ: определить оптимальный размер скидок по категориям | Визуализировать влияние скидок на продажи |
| 17 | Анализ рекламы: загрузить advertising.csv в HDFS, оценить эффективность каналов | SQL-анализ: рассчитать стоимость привлечения клиента по каналам | Построить диаграмму распределения бюджета |
| 18 | Анализ ассортимента: загрузить assortment.csv в HDFS, оптимизировать структуру | SQL-анализ: определить наиболее и наименее прибыльные товары | Визуализировать структуру ассортимента |
| 19 | Анализ персонала: загрузить employees.csv в HDFS, оценить эффективность работы | SQL-анализ: рассчитать производительность и выработку по сотрудникам | Построить рейтинг эффективности персонала |
| 20 | Анализ филиалов: загрузить branches.csv в HDFS, сравнить показатели филиалов | SQL-анализ: определить рентабельность и оборот по филиалам | Визуализировать географию продаж |
| 21 | Анализ конкурентов: загрузить competitors.csv в HDFS, провести сравнительный анализ | SQL-анализ: сравнить цены и ассортимент с конкурентами | Построить карту конкурентного окружения |
| 22 | Анализ затрат: загрузить costs.csv в HDFS, определить структуру расходов | SQL-анализ: рассчитать себестоимость по категориям товаров | Визуализировать структуру затрат |
| 23 | Анализ качества: загрузить quality.csv в HDFS, выявить проблемные товары | SQL-анализ: определить процент брака по производителям | Построить диаграмму качества продукции |
| 24 | Анализ закупок: загрузить purchases.csv в HDFS, оптимизировать закупки | SQL-анализ: рассчитать оптимальный размер закупки по товарам | Визуализировать структуру закупок |
| 25 | Анализ онлайн-оплат: загрузить payments.csv в HDFS, исследовать платежи | SQL-анализ: определить популярные способы оплаты и средний чек | Построить диаграмму способов оплаты |
| 26 | Анализ гарантий: загрузить warranty.csv в HDFS, оценить гарантийные случаи | SQL-анализ: рассчитать процент гарантийных обращений по товарам | Визуализировать гарантийные случаи |
| 27 | Анализ экспорта: загрузить export.csv в HDFS, исследовать международные продажи | SQL-анализ: определить наиболее прибыльные направления экспорта | Построить карту экспортных продаж |
| 28 | Анализ веб-трафика: загрузить traffic.csv в HDFS, исследовать поведение пользователей | SQL-анализ: рассчитать конверсию по источникам трафика | Визуализировать воронку продаж |
| 29 | Анализ оборудования: загрузить equipment.csv в HDFS, оценить эффективность использования | SQL-анализ: рассчитать загрузку и простои оборудования | Построить график загрузки мощностей |
| 30 | Анализ документооборота: загрузить documents.csv в HDFS, оптимизировать процессы | SQL-анализ: определить среднее время обработки документов | Визуализировать процесс документооборота |

### Общие требования к выполнению:

1. Для первого задания использовать распределенную обработку данных на Spark + Hadoop:
   - Загрузка данных в HDFS
   - Применение операций Spark для анализа
   - Использование RDD и DataFrame API

2. Для второго задания использовать локальный режим Spark и SparkSQL:
   - Создание временных таблиц
   - Написание SQL-запросов для анализа
   - Агрегация и группировка данных

3. Для третьего задания использовать библиотеки визуализации:
   - matplotlib
   - seaborn
   - Создание информативных графиков и диаграмм
   - Оформление легенд и подписей


### Правила оформления отчета

1. **Название репозитория:**
   Репозиторий должен иметь понятное и лаконичное название, отражающее содержание работы, например: `spark-data-processing-lab1`.

2. **Структура репозитория:**
   В репозитории должны быть следующие файлы:
   - **Код** — основной файл с кодом решения задачи в формате `.py` или `.ipynb`. Все используемые библиотеки должны быть импортированы в коде.
   - **Отчет** — файл с отчетом, оформленный в формате Markdown (например, `report.md`) или PDF, который включает:
     - Введение (цель и задачи работы);
     - Описание методов и шагов, предпринятых для выполнения работы;
     - Пример кода с объяснением ключевых моментов;
     - Анализ полученных результатов;
     - Выводы.
   - **README.md** — файл с общей информацией о репозитории, инструкции по установке и запуску кода, краткое описание работы.

3. **Стиль кода:**
   - Все комментарии в коде должны быть лаконичными и пояснять, что делает каждая часть кода.
   - Код должен быть структурирован, использовать отступы и разделение на логические блоки.
   - Функции и переменные должны иметь осмысленные и описательные имена.

4. **Отчет:**
   - В отчете должно быть описание всех выполненных этапов работы, включая данные, методы обработки и полученные результаты.
   - Для каждой задачи следует предоставить фрагменты кода и краткие пояснения.
   - Включить визуализации результатов (графики, таблицы), если это применимо.
   - В конце отчета должны быть сделаны выводы о выполненной работе.

5. **Ссылка на репозиторий в Moodle:**
   После завершения работы и размещения кода и отчета в репозитории, предоставьте ссылку на репозиторий в Moodle. Убедитесь, что репозиторий является публичным или доступен для преподавателя.
