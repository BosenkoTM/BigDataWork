## Практическая работа 1.2. Обработка данных с использованием Apache Spark и Python (PySpark)
Цель: освоение основ работы с Apache Spark и его интеграцией с Python через библиотеку PySpark. Студенты научатся обрабатывать большие объемы данных, используя распределенные вычисления, а также научатся применять базовые операции с RDD (Resilient Distributed Datasets) и DataFrame, работать с SQL-запросами в Spark SQL, а также визуализировать результаты обработки данных.

### Задачи:
1. **Установить Apache Spark и PySpark.**  
   Настроить рабочую среду для использования PySpark в Python, установить необходимые зависимости и настроить Spark на локальном компьютере или через облачную платформу.

2. **Загрузка данных и их предварительная обработка.**  
   Скачать или подготовить исходные данные для анализа (например, текстовые файлы или CSV). Загружать данные в Spark через RDD или DataFrame, выполнить предварительную обработку: очистка данных, фильтрация, преобразования.

3. **Применение операций с RDD и DataFrame.**  
   Научиться работать с RDD и DataFrame, выполнять такие операции как map, filter, reduce, groupBy, join и другие стандартные операции для обработки данных в распределенной среде.

4. **Применение SQL-запросов через Spark SQL.**  
   Использование SQL-запросов в Spark для извлечения и агрегации данных, создание временных таблиц и выполнение сложных запросов для анализа данных.

5. **Визуализация результатов анализа данных.**  
   Визуализировать полученные результаты с помощью библиотеки Python для визуализации данных (например, matplotlib или seaborn). Построить графики для лучшего представления результатов.

6. **Подготовка отчета.**  
   Оформить отчет по выполненной практике, в котором будет описан процесс выполнения работы, анализ полученных результатов и выводы. Включить ссылки на репозиторий и прикрепить сам отчет в формате PDF или Markdown.

Алгоритм выполнения задания в Apache Hadoop

Развернуть образ конфигурации  [ds_mgpu_Hadoop3+spark_3_4](http://95.31.0.249/moodle/mod/assign/view.php?id=1357) 

Все дальнейшие действия выполняются пользователем hadoop.
```bash
sudo su - hadoop
```

Шаг 1. Запуск Hadoop.
```bash
start-dfs.sh
```

```bash
start-yarn.sh
```
Шаг 2. Проверка работы Hadoop.
```bash
jps
```

В стандартной конфигурации Hadoop HDFS предоставляет веб-интерфейсы:
-	HDFS NameNode: http://localhost:9870 
-	YARN ResourceManager: http://localhost:8088

установить разрешение на запись для всех пользователей

```bash
hdfs dfs -chmod 775 /user2/hadoop/economic_data
```

разрешить запись только пользователю devops

```bash
hdfs dfs -setfacl -m user:devops:rwx /user3/hadoop/economic_data

Для того чтобы загрузить все файлы из локальной папки `~/Downloads/lab_1_2/data` в HDFS в папку `sparkdir`, выполните следующие шаги:

### Шаг 1. Создание папки в HDFS

Для начала создадим каталог `sparkdir` в HDFS:

```bash
hdfs dfs -mkdir -p /user5/sparkdir
```

Замените `devops` на имя пользователя, если это необходимо.

### Шаг 2. Загрузка файлов в HDFS

Теперь, когда папка `sparkdir` создана, нужно загрузить все файлы из локальной директории в HDFS.

Для этого используйте команду `hdfs dfs -put`:

```bash
hdfs dfs -put /home/hadoop/Downloads/lab_01_2/lab_1_2/data/* /user5/sparkdir/
```

Эта команда загрузит все файлы из папки `/home/hadoop/Downloads/lab_01_2/lab_1_2/data/` в папку `sparkdir` на HDFS.

### Шаг 3. Проверка загрузки

Чтобы убедиться, что файлы были успешно загружены, выполните команду:

```bash
hdfs dfs -ls /user5/sparkdir/
```

Эта команда выведет список всех файлов в каталоге `sparkdir` на HDFS.

Теперь все файлы из `~/Downloads/lab_1_2/data` должны быть успешно загружены в `sparkdir` на HDFS.


Завершение работы с Hadoop

```bash
stop-yarn.sh
```

```bash
stop-dfs.sh
```

Для полной остановки всех Hadoop-демонов:

```bash
stop-all.sh
```

Проверка остановки всех процессов:

```bash
jps
```

## Индивидуальное задание

# Варианты заданий по теме бизнес-анализа на основе Apache Spark и PySpark

# Варианты заданий по теме бизнес-анализа на основе Apache Spark и PySpark

| Задание № | Описание задания | Технология/Режим | Детали |
|-----------|------------------|------------------|--------|
| 1 | **Настройка Hadoop и Spark для бизнес-анализа** | Spark + Hadoop | Развертывание и настройка конфигурации Hadoop и Spark для обработки данных. |
| 2 | **WordCount с использованием Spark и Hadoop** | Spark + Hadoop | Подсчет частоты слов в текстовом файле, размещенном в HDFS. |
| 3 | **Визуализация данных WordCount** | Python (matplotlib, seaborn) | Построение графика распределения топ-10 наиболее частых слов. |
| 4 | **Настройка и запуск Spark Local для обработки данных** | Spark Local | Запуск локальной среды Spark для обработки данных из текстовых файлов. |
| 5 | **Подсчет частоты слов с использованием Spark Local** | Spark Local | Подсчет частоты слов в текстовом файле с использованием локального режима Spark. |
| 6 | **Визуализация частоты слов в локальном режиме** | Python (matplotlib) | Визуализация топ-10 наиболее частых слов с использованием бар-чарта. |
| 7 | **SQL-запросы с использованием Spark и Hadoop** | Spark + Hadoop | Выполнение SQL-запросов для агрегации и анализа данных из HDFS (например, вычисление среднего дохода по регионам). |
| 8 | **SQL-запросы с использованием Spark Local** | Spark Local | Выполнение SQL-запросов для анализа и агрегации данных из локальных CSV-файлов. |
| 9 | **Визуализация распределения данных по регионам с использованием SQL-запросов** | Python (seaborn) | Визуализация распределения данных по регионам с использованием boxplot. |
| 10 | **Обработка данных с использованием функций окна в Spark SQL** | Spark SQL | Применение оконных функций для вычисления рангов, скользящих средних и других метрик. |
| 11 | **Анализ дохода по регионам с использованием SQL-запросов** | Spark SQL | Выполнение SQL-запросов для расчета дохода по регионам и странам. |
| 12 | **Визуализация доходов по регионам с использованием seaborn** | Python (seaborn) | Визуализация данных по доходам в разных регионах на графике. |
| 13 | **Моделирование с использованием Spark MLlib** | Spark MLlib | Построение моделей машинного обучения (например, классификация) для прогнозирования бизнес-метрик. |
| 14 | **Применение классификации с использованием Spark MLlib** | Spark MLlib | Реализация классификации с использованием алгоритма RandomForest или другого классификатора. |
| 15 | **Анализ результатов классификации с визуализацией** | Python (matplotlib) | Визуализация результатов классификации на графиках (например, ROC-кривые, важность признаков). |
| 16 | **Применение алгоритмов кластеризации для анализа данных** | Spark MLlib | Реализация кластеризации с использованием алгоритма KMeans для сегментации данных. |
| 17 | **Визуализация кластеров и анализ результатов кластеризации** | Python (matplotlib) | Визуализация кластеров на графиках с использованием цветовых группировок. |
| 18 | **Работа с большими данными с использованием Spark** | Spark + Hadoop | Обработка больших данных, использование функций Spark для параллельной обработки. |
| 19 | **Визуализация частоты слов с использованием Spark RDD** | Spark RDD | Подсчет частоты слов в текстовом файле с использованием RDD и визуализация результатов. |
| 20 | **Обработка текстовых данных с использованием PySpark** | PySpark | Работа с большими текстовыми данными, очистка и обработка текстов с использованием PySpark. |
| 21 | **Анализ временных рядов с использованием PySpark** | PySpark | Применение анализа временных рядов для анализа продаж или других бизнес-метрик. |
| 22 | **Визуализация временных рядов с использованием seaborn** | Python (seaborn) | Визуализация трендов данных во времени с использованием графиков. |
| 23 | **Обработка CSV-файлов с использованием Spark** | Spark SQL | Загрузка, очистка и анализ данных из CSV-файлов с использованием PySpark. |
| 24 | **Применение функций агрегации с использованием Spark SQL** | Spark SQL | Применение агрегатных функций для вычисления сумм, средних значений и других метрик. |
| 25 | **Применение оптимизаций для работы с большими данными в Spark** | Spark + Hadoop | Использование кэширования, партиционирования и других методов оптимизации для обработки больших данных. |
| 26 | **Применение PySpark для обработки больших текстовых данных** | PySpark | Обработка больших коллекций текстовых данных с использованием RDD и DataFrame. |
| 27 | **Анализ бизнес-данных с использованием SQL-запросов** | Spark SQL | Выполнение SQL-запросов для агрегации и анализа данных с бизнес-метками. |
| 28 | **Применение статистического анализа для бизнес-данных** | PySpark + Python | Применение статистических методов для анализа распределений и выявления закономерностей в данных. |
| 29 | **Визуализация аналитических результатов с использованием matplotlib** | Python (matplotlib) | Построение графиков и диаграмм для представления бизнес-результатов. |
| 30 | **Создание отчетов и анализ с использованием данных Spark** | PySpark + Python | Составление отчетов с анализом полученных данных и их визуализацией. |

### Правила оформления отчета

1. **Название репозитория:**
   Репозиторий должен иметь понятное и лаконичное название, отражающее содержание работы, например: `spark-data-processing-lab1`.

2. **Структура репозитория:**
   В репозитории должны быть следующие файлы:
   - **Код** — основной файл с кодом решения задачи в формате `.py` или `.ipynb`. Все используемые библиотеки должны быть импортированы в коде.
   - **Отчет** — файл с отчетом, оформленный в формате Markdown (например, `report.md`) или PDF, который включает:
     - Введение (цель и задачи работы);
     - Описание методов и шагов, предпринятых для выполнения работы;
     - Пример кода с объяснением ключевых моментов;
     - Анализ полученных результатов;
     - Выводы.
   - **README.md** — файл с общей информацией о репозитории, инструкции по установке и запуску кода, краткое описание работы.

3. **Стиль кода:**
   - Все комментарии в коде должны быть лаконичными и пояснять, что делает каждая часть кода.
   - Код должен быть структурирован, использовать отступы и разделение на логические блоки.
   - Функции и переменные должны иметь осмысленные и описательные имена.

4. **Отчет:**
   - В отчете должно быть описание всех выполненных этапов работы, включая данные, методы обработки и полученные результаты.
   - Для каждой задачи следует предоставить фрагменты кода и краткие пояснения.
   - Включить визуализации результатов (графики, таблицы), если это применимо.
   - В конце отчета должны быть сделаны выводы о выполненной работе.

5. **Ссылка на репозиторий в Moodle:**
   После завершения работы и размещения кода и отчета в репозитории, предоставьте ссылку на репозиторий в Moodle. Убедитесь, что репозиторий является публичным или доступен для преподавателя.
