{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# spark wordcount 1"
      ],
      "metadata": {
        "id": "unDrVwsLSEjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "FX0FFqzgSBMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "YxSfxygKSf_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import pyspark\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import udf, isnan, min, max, sum, count, desc, expr, avg\n",
        "from pyspark.sql.types import IntegerType, LongType\n",
        "\n",
        "from pyspark.ml.feature import StandardScaler, VectorAssembler, MinMaxScaler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, LogisticRegression, GBTClassifier, LogisticRegressionModel, GBTClassificationModel, RandomForestClassificationModel\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pj4QwpYkSg3C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MQuczMZrRZ8t"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "# creating spark context\n",
        "sc = SparkContext('local', 'WordCount App')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YUmfGerBRZ8x"
      },
      "outputs": [],
      "source": [
        "# loading text from file\n",
        "with open('idiot.txt') as src:\n",
        "    text = src.readlines()\n",
        "\n",
        "# making RDD from text lines\n",
        "text_rdd = sc.parallelize(text)\n",
        "\n",
        "# counting word entries and storing result as RDD\n",
        "wc_rdd = text_rdd.flatMap(lambda line: line.split()) \\\n",
        "                    .map(lambda word: (word, 1)) \\\n",
        "                    .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# output 10 most frequent words\n",
        "most_frequent = wc_rdd.map(lambda e: e[::-1]).top(10)\n",
        "print(most_frequent)\n",
        "\n",
        "# # getting result back to client\n",
        "wc = wc_rdd.collect()\n",
        "#\n",
        " # output 10 most frequent words\n",
        "print(sorted(wc, key=lambda e: -e[-1])[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sparksql"
      ],
      "metadata": {
        "id": "I_COL05hTRWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "dJzEUKytTker"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "            .builder \\\n",
        "            .master('local') \\\n",
        "            .appName('SQL App').getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "bc_CieJYTUeM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat sales_header.csv"
      ],
      "metadata": {
        "id": "wl5mVCT8TY57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_row(line):\n",
        "    # TODO: parse row values\n",
        "    return line.split(',')\n",
        "\n",
        "sales_rdd = sc.textFile('sales.csv').map(parse_row)"
      ],
      "metadata": {
        "id": "h3-wkF9CT3nI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sales_header.csv') as src:\n",
        "    sales_header = src.read().strip().split(',')\n",
        "\n",
        "sales_df = sales_rdd.toDF(sales_header)\n",
        "sales_df.registerTempTable('sales')\n",
        "\n",
        "sales_total_df = spark.sql(\"SELECT COUNT(1) FROM sales\")\n",
        "print(sales_total_df.rdd.collect())\n",
        "\n",
        "# TODO: print revenue by Country and State"
      ],
      "metadata": {
        "id": "gpmCKqszT9Bs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "unDrVwsLSEjz",
        "I_COL05hTRWz"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}