{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unDrVwsLSEjz"
   },
   "source": [
    "# spark wordcount 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxSfxygKSf_s"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pj4QwpYkSg3C"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf, isnan, min, max, sum, count, desc, expr, avg\n",
    "from pyspark.sql.types import IntegerType, LongType\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, LogisticRegression, GBTClassifier, LogisticRegressionModel, GBTClassificationModel, RandomForestClassificationModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re  # Добавляем импорт модуля re\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount App\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Установка количества разделов для shuffle операций\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "\n",
    "# Чтение данных из HDFS (текстовый файл)\n",
    "file_path = \"hdfs://localhost:9000/user5/sparkdir/idiot.txt\"\n",
    "df = spark.read.text(file_path)  # Используем .text, так как это текстовый файл\n",
    "\n",
    "# Печать первых нескольких строк\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование текста в RDD для подсчета частоты слов\n",
    "text_rdd = df.rdd.flatMap(lambda row: row[0].split())\n",
    "\n",
    "def clean_word(word):\n",
    "    # Приводим слово к нижнему регистру\n",
    "    word = word.lower()\n",
    "    # Убираем все спецсимволы и цифры\n",
    "    word = re.sub(r'[^a-zA-Zа-яА-ЯёЁ]', '', word)  # Исправлено регулярное выражение\n",
    "    # Убираем начальные и конечные пробелы\n",
    "    word = word.strip()\n",
    "    return word\n",
    "\n",
    "# Очистка и фильтрация слов\n",
    "text_rdd_cleaned = (text_rdd\n",
    "                    .filter(lambda x: x is not None)\n",
    "                    .map(lambda x: clean_word(x))  # Используем функцию очистки\n",
    "                    .filter(lambda x: len(x) > 3)  # Убираем слова длиной 3 и менее\n",
    "                    .filter(lambda x: len(x) > 0)  # Убираем пустые строки\n",
    "                    .collect())\n",
    "\n",
    "# Подсчет частоты слов\n",
    "word_counts = (spark.sparkContext.parallelize(text_rdd_cleaned)  # Используем spark.sparkContext для создания RDD\n",
    "               .map(lambda word: (word, 1))\n",
    "               .reduceByKey(lambda x, y: x + y)\n",
    "               .filter(lambda x: x[1] > 1))  # Оставляем только слова, встречающиеся более одного раза\n",
    "\n",
    "# Преобразуем в DataFrame\n",
    "word_counts_df = spark.createDataFrame(word_counts, [\"word\", \"count\"])\n",
    "\n",
    "# Сортируем по убыванию частоты и выбираем топ-10\n",
    "top_10_words = word_counts_df.orderBy(\"count\", ascending=False).limit(10)\n",
    "\n",
    "# Создаем красивую таблицу с помощью pandas\n",
    "table_data = top_10_words.toPandas()\n",
    "print(\"Топ-10 наиболее частых слов:\")\n",
    "print(table_data.to_string(index=False))\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=table_data,\n",
    "    x='count',\n",
    "    y='word',\n",
    "    hue='word',  # Явно указываем hue для устранения предупреждения\n",
    "    palette='viridis',\n",
    "    legend=False  # Отключаем легенду\n",
    ")\n",
    "plt.title('Топ-10 наиболее часто встречающихся слов', pad=20)\n",
    "plt.xlabel('Частота')\n",
    "plt.ylabel('Слово')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_COL05hTRWz"
   },
   "source": [
    "# sparksql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQL App\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Установка количества разделов для shuffle операций\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "\n",
    "# Чтение данных из HDFS (текстовый файл)\n",
    "file_path = \"hdfs://localhost:9000/user5/sparkdir/sales.csv\"\n",
    "header_path = \"hdfs://localhost:9000/user5/sparkdir/sales_header.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(line):\n",
    "    # Парсим строку (разделение по запятой)\n",
    "    return line.split(',')\n",
    "\n",
    "# Чтение данных с использованием RDD\n",
    "sales_rdd = spark.sparkContext.textFile(file_path).map(parse_row)\n",
    "\n",
    "# Чтение заголовка из HDFS\n",
    "sales_header_rdd = spark.sparkContext.textFile(header_path).take(1)  # Читаем первую строку заголовка\n",
    "sales_header = sales_header_rdd[0].split(',')  # Разделяем по запятой\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3-wkF9CT3nI"
   },
   "outputs": [],
   "source": [
    "# Преобразуем RDD в DataFrame с заголовками\n",
    "sales_df = spark.createDataFrame(sales_rdd, sales_header)\n",
    "\n",
    "# Регистрация временной таблицы\n",
    "sales_df.createOrReplaceTempView('sales')\n",
    "\n",
    "# Выполнение SQL-запроса для получения всех данных\n",
    "sales_total_df = spark.sql(\"SELECT * FROM sales\")\n",
    "\n",
    "# Печать первых 10 строк\n",
    "sales_total_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpmCKqszT9Bs"
   },
   "outputs": [],
   "source": [
    "# Визуализация распределения цен по странам\n",
    "# Преобразуем DataFrame в pandas для визуализации\n",
    "sales_total_df_pd = sales_total_df.toPandas()\n",
    "\n",
    "# Построение графика для анализа цен по странам\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    data=sales_total_df_pd,\n",
    "    x='Country',  # Страна\n",
    "    y='Price',    # Цена\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "plt.title('Распределение цен по странам')\n",
    "plt.xlabel('Страна')\n",
    "plt.ylabel('Цена')\n",
    "plt.xticks(rotation=90)  # Поворачиваем подписи на оси X для лучшей читаемости\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Вывод дохода по стране и штату\n",
    "revenue_by_country_state = spark.sql(\"\"\"\n",
    "    SELECT Country, State, SUM(Price) as Total_Revenue\n",
    "    FROM sales\n",
    "    GROUP BY Country, State\n",
    "    ORDER BY Total_Revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "# Отображаем результаты\n",
    "revenue_by_country_state.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "unDrVwsLSEjz",
    "I_COL05hTRWz"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
