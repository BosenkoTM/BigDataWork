{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8-7dvE_Zm4g"
      },
      "source": [
        "# Как работает логистическая регрессия в Spark: особенности прогноза\n",
        "`Логистическая регрессия` – это статистическая модель, которая используется в машинном обучении для прогнозирования вероятности возникновения некоторого события путем построения логистической функции и сравнения этого события с кривой этой функции. В результате формируется ответ в виде вероятности бинарного события: `0` и `1`, где `0` – событие не произошло, `1` – событие произошло."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI5z3-gpZ5dY"
      },
      "source": [
        "# Работа с логистической регрессией в Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-SV-XNKZ86-"
      },
      "source": [
        "Для того, чтобы начать работу по прогнозу данных, необходимо настроить базовую конфигурацию, импортировав некоторые классы библиотек `Spark MLlib` и `Spark SQL`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahxmpdr9WKfO",
        "outputId": "6d1209f3-4f2b-4d77-d65c-3a94630a26e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GdxKYFiibmR",
        "outputId": "2d07efe9-b3d9-4e8e-8783-d8ef163ae013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JRmnDQnjaEEF"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzuSEEL0aJaO"
      },
      "source": [
        "ДАТАСЕТ С ДОМАМИ НА ПРОДАЖУ\n",
        "\n",
        "В качестве примера мы будем использовать датасет Kaggle, который содержит данные о домах на продажу в Бруклине с 2003 по 2017 года и доступен для скачивания. Он содержит 111 атрибутов (столбцов) и 390883 записей (строк). В атрибуты включены: дата продажи, дата постройки, цена на дом, налоговый класс, соседние регионы, долгота, ширина и др."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGNsiOjTb_Hi",
        "outputId": "e92d6d00-dc92-462a-f3e2-034c40097e5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-gPSOFi3cqTy"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCbI0XC6c1pP",
        "outputId": "0a29ff57-7ab9-47d6-bd42-0ef928b0967a"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/обработка больших данных/apache_spark/spark_seminar/data\")\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtNsTpspdJyf"
      },
      "source": [
        "Теперь необходимо импортировать входные данные, создав на их основе набор RDD (Resilient Distributed Dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C79tyQemiMr6"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "data = spark.read.csv(\n",
        "    '/content/drive/My Drive/Colab Notebooks/обработка больших данных/apache_spark/spark_seminar/data/brooklyn_sales_map.csv',\n",
        "    inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6c22CCGjHOJ"
      },
      "source": [
        "# ГОТОВИМ АТРИБУТ ДЛЯ ПОСЛЕДУЮЩЕЙ БИНАРНОЙ КЛАССИФИКАЦИИ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_4Ey9mWjVYq"
      },
      "source": [
        "Допустим, требуется классифицировать налоговый класс на дом (tax_class). Всего имеется 10 таких классов. Поскольку данные распределены неравномерно (например, в классе 1 имеется 198969 записей, а в 3-м — только 18), мы разделим их на 2 категории: те, которые принадлежат классу 1, и остальные. В Python это делается очень просто, нужно просто вызвать метод replace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j6md83Z5dcQH"
      },
      "outputs": [],
      "source": [
        "by_1 = ['1', '1A', '1B', '1C']\n",
        "by_others = ['2', '2A', '2B', '2C', '3', '4']\n",
        "data = data.replace(by_others, '0', ['tax_class'])\n",
        "data = data.replace(by_1, '1', ['tax_class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfDeelSOdriH"
      },
      "source": [
        "Кроме того, алгоритмы Machine Learning в PySpark работают с числовым значениями, а не со строками. Поэтому преобразуем значения столбца tax_class в тип int:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EjBKuI4PB9FJ"
      },
      "outputs": [],
      "source": [
        "data = data.withColumn('tax_class', data.tax_class.cast('int'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjuvxzFCjkbz"
      },
      "source": [
        "# ПОДБОР ПРИЗНАКОВ И ПРЕОБРАЗОВАНИЕ КАТЕГОРИЙ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM2ys_bBjnhC"
      },
      "source": [
        "Выберем следующие признаки для обучения модели `Machine Learning`: год постройки (**year_of_sale**), цена на дом (**sale_price**) и соседние регионы (**neighborhood**). Последние атрибут является категориальным признаком — в данных имеется `20` соседних регионов. Но опять же все значения этих категорий являются строковыми, поэтому нужно преобразовать их в числовые.\n",
        "\n",
        "Можно воспользоваться методом replace, как это сделано выше, но придётся сначала извлечь названия всех `20` регионов. А можно использовать специальный класс `StringIndexer`` из PySpark-модуля ML`, который выполнит за нас всю работу. Объект этого класса принимает в качестве аргументов: название атрибута, который нужно преобразовать (inputCol), и название, которое будет иметь преобразованный атрибут (outputCol). Вот так это выглядит в Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JY6rcIPsdsvH"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"neighborhood\", outputCol=\"neighborhood_id\")\n",
        "data = indexer.fit(data).transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9_bHiV1kCmi"
      },
      "source": [
        "Преобразованные категории имеют вид:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoY2t2DXVsfL",
        "outputId": "cce8c890-2a18-40a5-9dcb-0e5a60880f03"
      },
      "outputs": [],
      "source": [
        "data.groupBy('neighborhood_id').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c8_i4nbkMLy"
      },
      "source": [
        "Теперь выберем необходимые признаки, а также отбросим строки с пустыми значениями с помощью метода dropna в PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TUmvGdttVsfO"
      },
      "outputs": [],
      "source": [
        "features = ['year_of_sale', 'sale_price', 'neighborhood_id']\n",
        "target = 'tax_class'\n",
        "attributes = features + [target]\n",
        "sample = data.select(attributes).dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ST1HmjkRcD"
      },
      "source": [
        "# ВЕКТОРИЗАЦИЯ ПРИЗНАКОВ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P39-DBQ3kU5K"
      },
      "source": [
        "Поскольку алгоритмы машинного обучения в PySpark принимают на вход только вектора, то нужно  провести векторизацию. Для преобразования признаков в вектора используется класс VectorAssembler. Объект этого класса принимает в качестве аргументов список с названиями признаков, которые нужно векторизовать (inputCols), и название преобразованного признака (outputCol). После создания объекта VectorAssembler вызывается метод transform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3x1aHiokXRy"
      },
      "source": [
        "Для начала выберем в качестве признака для преобразования — цену на дом. Код на Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "AfvqCVuvVsfP"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['sale_price'],\n",
        "                            outputCol='features')\n",
        "output = assembler.transform(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hwpA8fTkf6i"
      },
      "source": [
        "Полученный после векторизации DataFrame выглядит следующим образом:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-VjtglAWvJ3",
        "outputId": "100f8f33-c187-42f8-9cf8-18d60be78615"
      },
      "outputs": [],
      "source": [
        "output.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoLLMchHklZy"
      },
      "source": [
        "# РАЗДЕЛЕНИЕ ДАТАСЕТА И ОБУЧЕНИЕ МОДЕЛИ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-q7zqsRkoR0"
      },
      "source": [
        "Для решения задач Machine Learning всегда нужно иметь, как минимум, две выборки — обучающую и тестовую. На обучающей мы будем обучать модель, а на тестовой проверять эффективность обученной модели. В PySpark сделать это очень просто, нужно просто вызвать метод randomSplit, который разделит исходный датасет в заданной пропорции. Мы разделим в пропорции 80:20, в Python это выглядит так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7-jD1yy2VsfP"
      },
      "outputs": [],
      "source": [
        "train, test = output.randomSplit([0.8, 0.2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcx8OJg2kvmr"
      },
      "source": [
        "Теперь воспользуемся логистической регрессией (Logistic Regression) [1], которая есть в PySpark, в качестве алгоритма Machine learning. Для этого нужно указать признаки, на которых модель обучается, и признак, который нужно классифицировать. Мы преобразовали цену на дом (sale price) в вектор под названием features, поэтому именно его и указываем в аргументе:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "w4YIukaEVsfP"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol='features',\n",
        "                        labelCol='tax_class')\n",
        "model = lr.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YvRbFJYk1v0"
      },
      "source": [
        "Осталось только получить предсказания. Для этого вызывается метод transform, который принимает тестовую выборку:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yBKwXT0MVsfQ"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMpH6OT3k7Br"
      },
      "source": [
        "Проверим эффективность модели, используя метрику качества. И в этом случае PySpark нас выручает, поскольку у него есть класс BinaryClassificationEvaluator. Нужно лишь указать целевой признак (tax class), а затем вызвать метод evaluate и передать в него наши предсказания. В Python это выглядит так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMven_DGVsfQ",
        "outputId": "37f9517b-7faf-41dd-d025-a117e0d7d6c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation: 0.5262600768248258\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='tax_class')\n",
        "print('Evaluation:', evaluator.evaluate(predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLtadnVzlDCz"
      },
      "source": [
        "Как видим, мы получили точность только 52%, что очень мало. Попробуем добавить ещё несколько признаков для обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDTFtqSPlDzU"
      },
      "source": [
        "# ДОБАВЛЕНИЕ ПРИЗНАКОВ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peyMWHZVlIdd"
      },
      "source": [
        "Векторизуем также год постройки (year_of_sale) и соседние регионы (neighborhood_id). Для этого нужно только в VectorAssembler указать выбранные признаки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hWfRlXT2lKAM"
      },
      "outputs": [],
      "source": [
        "features = ['year_of_sale', 'sale_price', 'neighborhood_id']\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features,\n",
        "                            outputCol='features')\n",
        "output = assembler.transform(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwWfd80olekr",
        "outputId": "619a0014-efcb-462d-e903-ebb75edfc4f7"
      },
      "outputs": [],
      "source": [
        "output.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qzT_E1RDlhGc"
      },
      "outputs": [],
      "source": [
        "train, test = output.randomSplit([0.8, 0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "moJ9dUDnllSD"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol='features',\n",
        "                        labelCol='tax_class')\n",
        "model = lr.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gW6NY04tlp-S"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiBky-C2lt_j",
        "outputId": "94921246-79aa-412e-a258-27c0648e999e"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='tax_class')\n",
        "print('Evaluation:', evaluator.evaluate(predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WVdVVU7lPU6"
      },
      "source": [
        "Python-код для остальных шагов — разделение на тестовую и обучающую выборки, обучение и оценивание модели — остаётся все тем же. В итоге, мы смогли повысить точность до 60%:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
