# Практическая работа №1. Обработка и анализ данных с использованием экосистемы Hadoop и Apache Spark (PySpark)

---

## Цель работы
Освоение основ работы с распределенной файловой системой HDFS и фреймворком Apache Spark. Студенты научатся разворачивать среду, загружать данные в HDFS, обрабатывать большие объемы данных с помощью PySpark (RDD, DataFrame), применять SQL-запросы для бизнес-аналитики и визуализировать результаты.

**Задачи:**
-  Развернуть и настроить Apache Hadoop и Spark (используя предоставленный образ или Colab).
-  Выполнить операции загрузки данных (ETL) в распределенную файловую систему.
-  Провести очистку и предобработку данных.
-  Выполнить аналитические запросы с использованием Spark SQL.
-  Визуализировать результаты для поддержки принятия управленческих решений.

---

## Необходимое ПО и окружение
Для выполнения работы рекомендуется использовать виртуальную машину с образом **`ds_mgpu_Hadoop3+spark_3_4`**.

**Технические характеристики:**
*   OS: Ubuntu 20.04 LTS (или новее).
*   Java: 8 или 11+.
*   Hadoop: 3.x.
*   Apache Spark: 3.4.3.
*   Python: 3.12+ с библиотекой `pyspark`.
*   JupyterHub / Jupyter Notebook.

*Альтернатива.* Google Colab (с установкой pyspark), однако приоритет отдается работе в настроенной локальной среде с HDFS.

---

## Алгоритм выполнения задания в Apache Hadoop(образ ds_mgpu_Hadoop3+spark_3_4)

Все действия по управлению кластером выполняются от пользователя `hadoop`.

### Шаг 1. Запуск кластера
В терминале выполните вход и запуск служб:
```bash
sudo su - hadoop
start-dfs.sh
start-yarn.sh
```

### Шаг 2. Проверка работы
Убедитесь, что процессы запущены (NameNode, DataNode, ResourceManager, NodeManager):
```bash
jps
```

**Веб-интерфейсы для мониторинга:**
*   HDFS NameNode: [http://localhost:9870](http://localhost:9870)
*   YARN ResourceManager: [http://localhost:8088](http://localhost:8088)

### Шаг 3. Подготовка HDFS и загрузка данных
Вам необходимо создать директорию в распределенной файловой системе и загрузить туда исходные данные (датасеты).

-  **Создание директорий:**
    ```bash
    # Создание папки пользователя (пример)
    hdfs dfs -mkdir -p /user/hadoop/lab_01/input
    
    # Настройка прав доступа (при необходимости)
    hdfs dfs -chmod 775 /user/hadoop/lab_01
    ```

-  **Загрузка данных:**
    Предположим, ваши скачанные датасеты (CSV) находятся локально в `~/Downloads/data`.
    ```bash
    hdfs dfs -put /home/hadoop/Downloads/data/*.csv /user/hadoop/lab_01/input/
    ```

-  **Проверка загрузки:**
    ```bash
    hdfs dfs -ls /user/hadoop/lab_01/input/
    ```

### Шаг 4. Завершение работы (после выполнения всех заданий)
```bash
stop-yarn.sh
stop-dfs.sh
# Или полная остановка
stop-all.sh
```

---

## Методические указания к PySpark

Работа выполняется в **Jupyter Notebook** или скрипте Python.

### Подключение и загрузка
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, desc

# Инициализация сессии
spark = SparkSession.builder \
    .appName("Lab1_BusinessInformatics") \
    .master("local[*]") \
    .getOrCreate()

# Загрузка данных из HDFS
# Обратите внимание на протокол hdfs:// и путь
df = spark.read.option("header", "true") \
    .option("inferSchema", "true") \
    .csv("hdfs://localhost:9000/user/hadoop/lab_01/input/ваш_файл.csv")

# Проверка схемы
df.printSchema()
df.show(5)
```

### Анализ и SQL
Для использования SQL-запросов необходимо зарегистрировать DataFrame как временное представление:
```python
df.createOrReplaceTempView("sales_data")

result = spark.sql("""
    SELECT category, SUM(amount) as total_revenue
    FROM sales_data
    GROUP BY category
    ORDER BY total_revenue DESC
""")
result.show()
```

---

## Варианты индивидуальных заданий

**Инструкция по выбору данных:**
Поскольку задание требует решения бизнес-кейса, вам необходимо найти подходящий датасет (Open Source) на ресурсах: Kaggle, GitHub, UCI Machine Learning Repository или сгенерировать синтетические данные, подходящие под описание варианта. Ссылку на источник данных обязательно указать в отчете. Также возможно согласовать датасет с преподавателем, если он соответствует предметной области специальности Бизнес-информатика.

**Вариант выбирается согласно номеру студента в списке группы (всего 40 вариантов).**

Задания на образовательном портале [IT-Adaptive](https://envlab.ru/mod/assign/view.php?id=457)

---

## Требования к отчетности

Для сдачи работы необходимо создать **публичный репозиторий** на GitHub или GitVerse.

### Структура репозитория:
-  **`README.md`**:
    *   Описание задачи и выбранного варианта.
    *   Инструкция по запуску.
    *   Ссылка на источник данных.
-  **`lab_01.ipynb`** (или `.py`):
    *   Основной код решения.
    *   Код должен быть структурирован, снабжен комментариями.
-  **`Report.md`** (или PDF):
    *   **Введение.** Цель, постановка бизнес-задачи, описание данных.
    *   **Ход работы:**
        *   Скриншоты загрузки данных в HDFS (команды консоли).
        *   Скриншоты/код предварительной обработки (schema, null checks).
    *   **Анализ:**
        *   SQL-запросы и таблицы с результатами.
        *   Графики (Визуализация) с интерпретацией (что означает этот график для бизнеса).
    *   **Выводы.** Итоги о применимости Hadoop/Spark для данной задачи.

**Сдача.** Ссылка на репозиторий отправляется в систему Moodle.


### Критерии оценки практической работы

| Категория | Критерий | Баллы |
| :--- | :--- | :--- |
| **1. Работа с HDFS и средой (Техническая часть)** | **3 балла** | |
| | Успешное развертывание среды, создание директорий в HDFS и загрузка исходных данных. Приведены доказательства (скриншоты консоли/вывод команд `hdfs dfs -ls`). | 1 |
| | Правильная настройка прав доступа и структуры папок согласно заданию. | 1 |
| | Данные корректно считаны из HDFS в Spark DataFrame (указан правильный URI `hdfs://...`). | 1 |
| **2. Реализация логики на PySpark и Spark SQL** | **3 баллов** | |
| **Задание 1 (ETL)** | Выполнена загрузка, очистка данных (обработка NULL, дубликатов), типизация колонок. Использованы методы Spark Core/DataFrame API. | 1 |
| **Задание 2 (SQL)** | Выполнен глубокий анализ данных с использованием Spark SQL. Написаны сложные запросы (агрегация, группировка, сортировка, фильтрация). Результаты соответствуют варианту задания. | 2 |
| **3. Визуализация и бизнес-интерпретация** | **2 баллов** | |
| **Задание 3 (Viz)** | Построены графики/диаграммы с использованием Matplotlib/Seaborn. Графики читаемы (есть легенда, подписи осей, заголовок). | 1 |
| **Аналитика** | Приведена интерпретация полученных результатов. Сделан вывод, полезный для бизнеса (ответ на вопрос "Что значат эти цифры?"). | 1 |
| **4. Оформление и культура кода** | **2 балла** | |
| **Репозиторий** | Репозиторий на GitHub/GitVerse создан корректно. Присутствует `README.md` с описанием проекта, инструкцией по запуску и ссылкой на источник данных. | 1 |
| **Отчет** | Отчет (PDF/Markdown) содержит введение, описание хода работы, код с пояснениями, скриншоты и итоговые выводы. Код структурирован, есть комментарии. | 1 |

| **Итого** | | **10** |


